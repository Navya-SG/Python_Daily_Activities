{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0293b878-6d83-4836-924a-fb66ce1ae568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 10, 25, 38, 45]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def insertion_sort(arr):\n",
    "    for i in range(1, len(arr)): \n",
    "        key = arr[i]\n",
    "        j = i - 1\n",
    "        while j >= 0 and arr[j] > key:\n",
    "            arr[j + 1] = arr[j]\n",
    "            j -= 1\n",
    "        arr[j + 1] = key\n",
    "    return arr\n",
    "\n",
    "arr = [25, 38, 10, 7, 45]\n",
    "insertion_sort(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53443bcc-6f45-426e-9555-9bba7d6dd2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 10, 25, 38, 45]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def selection_sort(arr):\n",
    "    for i in range(len(arr)):\n",
    "        min_idx = i\n",
    "        for j in range(i + 1, len(arr)):\n",
    "            if arr[j] < arr[min_idx]:\n",
    "                min_idx = j\n",
    "        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n",
    "    return arr\n",
    "arr = [25, 38, 10, 7, 45]\n",
    "selection_sort(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7a23851-fd8e-4e35-a19c-5f8135a321b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 10, 25, 38, 45]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quick_sort(arr):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    pivot = arr[len(arr) // 2]\n",
    "    left = [x for x in arr if x < pivot]\n",
    "    middle = [x for x in arr if x == pivot]\n",
    "    right = [x for x in arr if x > pivot]\n",
    "    return quick_sort(left) + middle + quick_sort(right)\n",
    "arr = [25, 38, 10, 7, 45]\n",
    "quick_sort(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "688e5012-3b87-4845-870e-069ee0972229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 10, 25, 38, 45]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bubble_sort(arr):\n",
    "    n = len(arr)\n",
    "    for i in range(n):\n",
    "        swapped = False\n",
    "        for j in range(0, n - i - 1):\n",
    "            if arr[j] > arr[j + 1]:\n",
    "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
    "                swapped = True\n",
    "        if not swapped:\n",
    "            break\n",
    "    return arr\n",
    "arr = [25, 38, 10, 7, 45]\n",
    "bubble_sort(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8ad079fa-2944-4f26-9865-5cdb4368438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter from [25, 38, 10, 7, 45] 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = left + (right - left) // 2  # Prevents overflow\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1  # Return -1 if not found\n",
    "arr = [25, 38, 10, 7, 45]\n",
    "arr.sort()\n",
    "target = int(input(\"enter from [25, 38, 10, 7, 45]\"))\n",
    "binary_search(arr,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a172955b-7ad9-4057-bd01-ea5e124a6bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter from [25, 38, 10, 7, 45] 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_search(arr, target):\n",
    "    for i, value in enumerate(arr):\n",
    "        if value == target:\n",
    "            return i  # Return index if found\n",
    "    return -1  # Return -1 if not found\n",
    "arr = [25, 38, 10, 7, 10]\n",
    "target = int(input(\"enter from [25, 38, 10, 7, 10]\"))\n",
    "linear_search(arr,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d10ef29d-1717-465a-94e9-8b0c38264a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target found at indices: [1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "def linear_search_all_occurrences(arr, target):\n",
    "    indices = []\n",
    "    for i, value in enumerate(arr):\n",
    "        if value == target:\n",
    "            indices.append(i)\n",
    "    return indices\n",
    "data = [5, 3, 7, 3, 9, 3, 2]\n",
    "target = 3\n",
    "result = linear_search_all_occurrences(data, target)\n",
    "print(\"Target found at indices:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c78ed2-15fd-4de3-90b9-0b4350673dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_files=[]\n",
    "combined=[]\n",
    "for file in json_files:\n",
    "    with open(file,'r') as jf:\n",
    "        data = json.load(jf)\n",
    "        if isinstance(data,list):\n",
    "            combined.extend(data)\n",
    "        else:\n",
    "            combined.append(data)\n",
    "with open(\"combined.json\",'w') as jwf:\n",
    "    json.dump(combined,jwf,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bd7c01-669f-4445-a511-441714c15d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_files=[]\n",
    "with open(\"combines\",'w',newline='') as cf:\n",
    "    writer=None\n",
    "\n",
    "    for f in csv_files:\n",
    "        with open(file,'r') as crf:\n",
    "            reader=csv.reader(crf)\n",
    "            header = next(reader)\n",
    "\n",
    "            if writer is None:\n",
    "                writer=csv.writer(cf)\n",
    "                writer.writerows(header)\n",
    "\n",
    "            for row in reader:\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352cc408-7c42-42da-9fe5-1828b8fd4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip(files,extract):\n",
    "    with zipfile.ZipFile(files,'r') as f:\n",
    "        list = f.namelist()\n",
    "        f.extractall(extract)\n",
    "        return list\n",
    "\n",
    "list = unzip(\"\",extract)\n",
    "\n",
    "for file in list:\n",
    "    unzip(file,extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514ec10-e8ff-4790-ae12-9a1da1e5ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        zf.extractall(extract_to)\n",
    "        return zf.namelist()\n",
    "\n",
    "# Step 1: Extract outer zip\n",
    "outer_zip = \"data_bundle.zip\"\n",
    "outer_folder = \"extracted_outer\"\n",
    "inner_files = extract_zip(outer_zip, outer_folder)\n",
    "\n",
    "# Step 2: Extract inner zips\n",
    "all_data_files = []\n",
    "for zip_name in inner_files:\n",
    "    zip_path = os.path.join(outer_folder, zip_name)\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        extract_folder = os.path.join(outer_folder, zip_name.replace(\".zip\", \"\"))\n",
    "        files = extract_zip(zip_path, extract_folder)\n",
    "        all_data_files.extend([os.path.join(extract_folder, f) for f in files])\n",
    "\n",
    "'''\n",
    "all_data_files = []\n",
    "\n",
    "# Loop through each inner zip file name\n",
    "for zip_name in inner_files:\n",
    "    # Full path to the inner zip file\n",
    "    zip_path = os.path.join(outer_folder, zip_name)\n",
    "\n",
    "    # Check if it's a valid zip file\n",
    "    if zipfile.is_zipfile(zip_path):\n",
    "        # Create a folder name to extract into (remove .zip from name)\n",
    "        extract_folder = zip_path.replace(\".zip\", \"\")\n",
    "\n",
    "        # Extract the inner zip file\n",
    "        extracted_files = extract_zip(zip_path, extract_folder)\n",
    "\n",
    "        # Add full paths of extracted files to the list\n",
    "        for file in extracted_files:\n",
    "            full_path = os.path.join(extract_folder, file)\n",
    "            all_data_files.append(full_path)\n",
    "'''\n",
    "# Step 3: Read and combine CSV and JSON\n",
    "csv_data = []\n",
    "json_data = []\n",
    "\n",
    "for file in all_data_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        csv_data.append(pd.read_csv(file))\n",
    "    elif file.endswith(\".json\"):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.json_normalize(data)\n",
    "            json_data.append(df)\n",
    "\n",
    "# Step 4: Combine and clean\n",
    "df_csv = pd.concat(csv_data, ignore_index=True)\n",
    "df_json = pd.concat(json_data, ignore_index=True)\n",
    "df_combined = pd.concat([df_csv, df_json], ignore_index=True)\n",
    "\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "df_combined.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Step 5: Groupby example\n",
    "if \"region\" in df_combined.columns:\n",
    "    print(df_combined.groupby(\"region\").size())\n",
    "elif \"customer_type\" in df_combined.columns:\n",
    "    print(df_combined.groupby(\"customer_type\").size())\n",
    "\n",
    "# Step 6: Save to zip\n",
    "df_combined.to_csv(\"final_combined.csv\", index=False)\n",
    "with zipfile.ZipFile(\"final_output.zip\", \"w\") as zf:\n",
    "    zf.write(\"final_combined.csv\")\n",
    "\n",
    "print(\"✅ Final output saved to final_output.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b36f40-4cdc-42fa-b5a6-e0919c5d35c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unzip(files,extract_to):\n",
    "    with zipfile.ZipFile(files,'r') as zf:\n",
    "        inner_files = zf.namelist()\n",
    "        zf.extractall(extract_to)\n",
    "        return inner_files\n",
    "\n",
    "inner_files = unzip(\"\",\"extracted\")\n",
    "\n",
    "for file in inner_files:\n",
    "    unzip(file,\"extract\")\n",
    "\n",
    "import json\n",
    "json_files=[]\n",
    "combined=[]\n",
    "for file in json_files:\n",
    "    with open(file,'r') as rjf:\n",
    "        data = json.load(rjf)\n",
    "        if isinstance(data,list):\n",
    "            combined.extend(data)\n",
    "        else:\n",
    "            combined.append(data)\n",
    "with open(\"combined_files.json\",'w') as jwf:\n",
    "    json.dump(combined,jwf,indent=2)\n",
    "\n",
    "import csv\n",
    "csv_files=[]\n",
    "with open(\"combined.csv\",'w',newline='') as f:\n",
    "    writer=None\n",
    "\n",
    "    for file in csv_files:\n",
    "        with open(file,r) as rf:\n",
    "            reader=csv.reader(rf)\n",
    "            header=next(reader)\n",
    "\n",
    "            if writer is None:\n",
    "                writer=csv.writer(f)\n",
    "                writer.writerows(headers)\n",
    "\n",
    "            for row in reader:\n",
    "                writer.write(row)\n",
    "\n",
    "csv_data = []\n",
    "json_data = []\n",
    "\n",
    "for file in all_data_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        csv_data.append(pd.read_csv(file))\n",
    "    elif file.endswith(\".json\"):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.json_normalize(data)\n",
    "            json_data.append(df)\n",
    "\n",
    "# Step 4: Combine and clean\n",
    "df_csv = pd.concat(csv_data, ignore_index=True)\n",
    "df_json = pd.concat(json_data, ignore_index=True)\n",
    "df_combined = pd.concat([df_csv, df_json], ignore_index=True)\n",
    "\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "df_combined.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Step 5: Groupby example\n",
    "if \"region\" in df_combined.columns:\n",
    "    print(df_combined.groupby(\"region\").size())\n",
    "elif \"customer_type\" in df_combined.columns:\n",
    "    print(df_combined.groupby(\"customer_type\").size())\n",
    "\n",
    "# Step 6: Save to zip\n",
    "df_combined.to_csv(\"final_combined.csv\", index=False)\n",
    "with zipfile.ZipFile(\"final_output.zip\", \"w\") as zf:\n",
    "    zf.write(\"final_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c07b02-7626-4619-ad2e-34572e7e7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas\n",
    "\n",
    "def unzip(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        sub_files = zf.namelist()\n",
    "        zf.extractall(extract_to)\n",
    "        return [os.path.join(extract_to, f) for f in file_list]\n",
    "\n",
    "def create_folder(folder_list):\n",
    "    for folder in folder_list:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def move_files(sub_files,json_folder,csv_folder):\n",
    "    for file_path in sub_zip_files:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        if file_path.endswith(\".json\"):\n",
    "            os.rename(file_path,os.path.join(json_folder,file_name))\n",
    "            csv_data.append(pd.read_json(file))\n",
    "        if file_path.endswith(\".csv\"):\n",
    "            os.rename(file_path,os.path.join(csv_folder,file_name))\n",
    "            csv_data.append(pd.read_csv(file))\n",
    "\n",
    "create_folder=['json_folder','csv_folder','main_extract_to','temp_extract']\n",
    "sub_zip_files = unzip(\"main.zip\", \"main_extract_to\")\n",
    "\n",
    "for zip_file in sub_zip_files:\n",
    "    extracted_files = unzip(zip_file,\"temp_extract\")\n",
    "    move_files(extracted_files,\"json_folder\",\"csv_folder\")\n",
    "\n",
    "for file in os.listdir(\"temp_extract\"):\n",
    "    os.remove(os.path.join(\"temp_extract\",file))\n",
    "os.rmdir(\"temp_extract\")\n",
    "\n",
    "for root, dirs, files in os.walk(\"main_extract_to\", topdown=False):\n",
    "    for name in files:\n",
    "        os.remove(os.path.join(root, name))\n",
    "    for name in dirs:\n",
    "        os.rmdir(os.path.join(root, name))\n",
    "os.rmdir(\"main_extract_to\")\n",
    "'''\n",
    "import json\n",
    "json_files=[]\n",
    "combined=[]\n",
    "for file in json_files:\n",
    "    with open(file,'r') as rjf:\n",
    "        data = json.load(rjf)\n",
    "        if isinstance(data,list):\n",
    "            combined.extend(data)\n",
    "        else:\n",
    "            combined.append(data)\n",
    "with open(\"combined_files.json\",'w') as jwf:\n",
    "    json.dump(combined,jwf,indent=2)\n",
    "\n",
    "import csv\n",
    "csv_files=[]\n",
    "with open(\"combined.csv\",'w',newline='') as f:\n",
    "    writer=None\n",
    "\n",
    "    for file in csv_files:\n",
    "        with open(file,r) as rf:\n",
    "            reader=csv.reader(rf)\n",
    "            header=next(reader)\n",
    "\n",
    "            if writer is None:\n",
    "                writer=csv.writer(f)\n",
    "                writer.writerows(headers)\n",
    "\n",
    "            for row in reader:\n",
    "                writer.write(row)\n",
    "'''\n",
    "df_csv = pd.concat(csv_data, ignore_index=True)\n",
    "df_json = pd.concat(json_data, ignore_index=True)\n",
    "df_combined = pd.concat([df_csv, df_json], ignore_index=True)\n",
    "\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "df_combined.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "if \"region\" in df_combined.columns:\n",
    "    print(df_combined.groupby(\"region\").size())\n",
    "elif \"customer_type\" in df_combined.columns:\n",
    "    print(df_combined.groupby(\"customer_type\").size())\n",
    "\n",
    "df_combined.to_csv(\"final_combined.csv\", index=False)\n",
    "with zipfile.ZipFile(\"final_output.zip\", \"w\") as zf:\n",
    "    zf.write(\"final_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1256df-6e2a-4b47-9b85-b61d9db1b29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Define helper functions\n",
    "def unzip(zip_path, extract_to):\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        file_list = zf.namelist()\n",
    "        zf.extractall(extract_to)\n",
    "        return [os.path.join(extract_to, f) for f in file_list]\n",
    "\n",
    "def create_folders(folder_list):\n",
    "    for folder in folder_list:\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "def move_files(file_paths, json_folder, csv_folder):\n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        if file_path.endswith(\".json\"):\n",
    "            os.rename(file_path, os.path.join(json_folder, file_name))\n",
    "        elif file_path.endswith(\".csv\"):\n",
    "            os.rename(file_path, os.path.join(csv_folder, file_name))\n",
    "\n",
    "# Step 2: Setup folders\n",
    "create_folders([\"json_folder\", \"csv_folder\", \"sub_temp_extract\", \"temp_extract\"])\n",
    "\n",
    "# Step 3: Unzip main zip\n",
    "main_zip = \"data_bundle.zip\"\n",
    "main_extract_to = \"sub_temp_extract\"\n",
    "inner_zip_files = unzip(main_zip, main_extract_to)\n",
    "\n",
    "# Step 4: Unzip each inner zip and move files\n",
    "for zip_file in inner_zip_files:\n",
    "    if zipfile.is_zipfile(zip_file):\n",
    "        extracted_files = unzip(zip_file, \"temp_extract\")\n",
    "        move_files(extracted_files, \"json_folder\", \"csv_folder\")\n",
    "\n",
    "# Step 5: Clean up temp folder\n",
    "for file in os.listdir(\"temp_extract\"):\n",
    "    os.remove(os.path.join(\"temp_extract\", file))\n",
    "os.rmdir(\"temp_extract\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c803606d-2cf3-4653-9361-1b11d8dc09a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Function to unzip files\n",
    "def unzip(zip_path, extract_to):\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
    "        inner_files = zf.namelist()\n",
    "        zf.extractall(extract_to)\n",
    "        return inner_files\n",
    "\n",
    "# Step 1: Extract outer zip\n",
    "outer_zip = \"data_bundle.zip\"\n",
    "outer_folder = \"extracted_outer\"\n",
    "inner_files = unzip(outer_zip, outer_folder)\n",
    "\n",
    "# Step 2: Extract inner zips\n",
    "all_data_files = []\n",
    "for file in inner_files:\n",
    "    inner_zip_path = os.path.join(outer_folder, file)\n",
    "    if zipfile.is_zipfile(inner_zip_path):\n",
    "        inner_folder = os.path.join(outer_folder, file.replace(\".zip\", \"\"))\n",
    "        extracted = unzip(inner_zip_path, inner_folder)\n",
    "        for f in extracted:\n",
    "            all_data_files.append(os.path.join(inner_folder, f))\n",
    "\n",
    "# Step 3: Combine JSON files\n",
    "json_data = []\n",
    "for file in all_data_files:\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(file, 'r') as rjf:\n",
    "            data = json.load(rjf)\n",
    "            if isinstance(data, list):\n",
    "                json_data.extend(data)\n",
    "            else:\n",
    "                json_data.append(data)\n",
    "\n",
    "with open(\"combined_files.json\", 'w') as jwf:\n",
    "    json.dump(json_data, jwf, indent=2)\n",
    "\n",
    "# Step 4: Combine CSV files using pandas\n",
    "csv_data = []\n",
    "for file in all_data_files:\n",
    "    if file.endswith(\".csv\"):\n",
    "        csv_data.append(pd.read_csv(file))\n",
    "\n",
    "# Step 5: Normalize JSON and combine with CSV\n",
    "df_csv = pd.concat(csv_data, ignore_index=True) if csv_data else pd.DataFrame()\n",
    "df_json = pd.json_normalize(json_data) if json_data else pd.DataFrame()\n",
    "df_combined = pd.concat([df_csv, df_json], ignore_index=True)\n",
    "\n",
    "# Step 6: Clean the data\n",
    "df_combined.drop_duplicates(inplace=True)\n",
    "df_combined.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Step 7: Groupby example\n",
    "if \"region\" in df_combined.columns:\n",
    "    print(\"Group by region:\")\n",
    "    print(df_combined.groupby(\"region\").size())\n",
    "elif \"customer_type\" in df_combined.columns:\n",
    "    print(\"Group by customer_type:\")\n",
    "    print(df_combined.groupby(\"customer_type\").size())\n",
    "\n",
    "# Step 8: Save final output\n",
    "df_combined.to_csv(\"final_combined.csv\", index=False)\n",
    "with zipfile.ZipFile(\"final_output.zip\", \"w\") as zf:\n",
    "    zf.write(\"final_combined.csv\")\n",
    "\n",
    "print(\"✅ Final output saved to final_output.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba1f661-c6d1-4835-9bb4-9a5e0b731548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Create a working directory\n",
    "os.makedirs(\"extracted_data\", exist_ok=True)\n",
    "\n",
    "# Step 1: Extract all files from data_bundle.zip and its inner zip files\n",
    "with zipfile.ZipFile(\"data_bundle.zip\", \"r\") as outer_zip:\n",
    "    outer_zip.extractall(\"extracted_data\")\n",
    "\n",
    "# Extract inner zip files\n",
    "for i in range(1, 5):\n",
    "    inner_zip_path = os.path.join(\"extracted_data\", f\"region_{i}.zip\")\n",
    "    with zipfile.ZipFile(inner_zip_path, \"r\") as inner_zip:\n",
    "        inner_zip.extractall(\"extracted_data\")\n",
    "\n",
    "# Step 2: Read all CSV files and combine them into one DataFrame\n",
    "csv_dataframes = []\n",
    "json_dataframes = []\n",
    "\n",
    "for root, dirs, files in os.walk(\"extracted_data\"):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if file.endswith(\".csv\"):\n",
    "            df_csv = pd.read_csv(file_path)\n",
    "            csv_dataframes.append(df_csv)\n",
    "        elif file.endswith(\".json\"):\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                df_json = pd.json_normalize(data)\n",
    "                json_dataframes.append(df_json)\n",
    "\n",
    "# Combine all CSV and JSON DataFrames\n",
    "csv_combined = pd.concat(csv_dataframes, ignore_index=True)\n",
    "json_combined = pd.concat(json_dataframes, ignore_index=True)\n",
    "\n",
    "# Step 3: Append JSON DataFrame to CSV DataFrame\n",
    "combined_df = pd.concat([csv_combined, json_combined], ignore_index=True)\n",
    "\n",
    "# Step 4: Remove duplicate rows\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Step 5: Fill missing values with \"Unknown\"\n",
    "combined_df.fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Step 6: Perform groupby operation on 'region' or 'customer_type'\n",
    "group_column = \"region\" if \"region\" in combined_df.columns else \"customer_type\"\n",
    "group_counts = combined_df.groupby(group_column).size()\n",
    "print(\"Group counts:\\n\", group_counts)\n",
    "\n",
    "# Step 7: Save the final DataFrame to a CSV file and compress it\n",
    "output_csv = \"final_output.csv\"\n",
    "combined_df.to_csv(output_csv, index=False)\n",
    "\n",
    "with zipfile.ZipFile(\"final_output.zip\", \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(output_csv)\n",
    "\n",
    "print(\"Final output saved to final_output.zip\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
