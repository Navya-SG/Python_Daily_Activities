{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00f932e0-a7b2-4240-b9e6-0c592d811775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53d52c58-7bdf-4cb7-aa60-95495015d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(main):\n",
    "    with zipfile.ZipFile(main,'r') as zf:\n",
    "        file_list = zf.namelist()\n",
    "        zf.extractall()\n",
    "    return file_list\n",
    "\n",
    "file_list = unzip(\"final_archive (2).zip\")\n",
    "\n",
    "for file in file_list:\n",
    "    unzip(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dac7d55b-d4cd-4a7d-a8fe-20621ce56090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.anaconda',\n",
       " '.conda',\n",
       " '.condarc',\n",
       " '.continuum',\n",
       " '.git',\n",
       " '.gitconfig',\n",
       " '.idlerc',\n",
       " '.ipynb_checkpoints',\n",
       " '.ipython',\n",
       " '.jupyter',\n",
       " '.lesshst',\n",
       " '.vscode',\n",
       " 'airline_analysis_outputs.zip',\n",
       " 'airline_analysis_outputs1.zip',\n",
       " 'airline_analysis_outputs2.zip',\n",
       " 'AppData',\n",
       " 'Application Data',\n",
       " 'combined.csv',\n",
       " 'combined.json',\n",
       " 'combined_airline_data.csv',\n",
       " 'Contacts',\n",
       " 'Cookies',\n",
       " 'csv_files',\n",
       " 'data.json',\n",
       " 'Documents',\n",
       " 'Downloads',\n",
       " 'Favorites',\n",
       " 'final_archive (2).zip',\n",
       " 'json_files',\n",
       " 'Links',\n",
       " 'Local Settings',\n",
       " 'm.salary)',\n",
       " 'main_extract',\n",
       " 'main_extratc',\n",
       " 'mini_ass.ipynb',\n",
       " 'mini_ass_final.ipynb',\n",
       " 'Music',\n",
       " 'My Documents',\n",
       " 'Navya_hello.ipynb',\n",
       " 'NetHood',\n",
       " 'new_airline_data_2024 (1).json',\n",
       " 'NTUSER.DAT',\n",
       " 'ntuser.dat.LOG1',\n",
       " 'ntuser.dat.LOG2',\n",
       " 'NTUSER.DAT{a4319a1f-4bf4-11f0-abc4-c32593279ea8}.TM.blf',\n",
       " 'NTUSER.DAT{a4319a1f-4bf4-11f0-abc4-c32593279ea8}.TMContainer00000000000000000001.regtrans-ms',\n",
       " 'NTUSER.DAT{a4319a1f-4bf4-11f0-abc4-c32593279ea8}.TMContainer00000000000000000002.regtrans-ms',\n",
       " 'ntuser.ini',\n",
       " 'old_airline_data_2023 (1).csv',\n",
       " 'OneDrive',\n",
       " 'OneDrive - Coforge Limited',\n",
       " 'Oracle',\n",
       " 'PrintHood',\n",
       " 'Recent',\n",
       " 'sample_1.csv',\n",
       " 'sample_1.json',\n",
       " 'sample_2.csv',\n",
       " 'sample_2.json',\n",
       " 'sample_3.csv',\n",
       " 'sample_3.json',\n",
       " 'sample_4.csv',\n",
       " 'sample_4.json',\n",
       " 'sample_data.zip',\n",
       " 'Saved Games',\n",
       " 'Searches',\n",
       " 'SendTo',\n",
       " 'Start Menu',\n",
       " 'sub_extract',\n",
       " 'target_not_achieved.csv',\n",
       " 'Templates',\n",
       " 'try',\n",
       " 'try.ipynb',\n",
       " 'try_1.ipynb',\n",
       " 'Untitled.ipynb',\n",
       " 'Untitled1.ipynb',\n",
       " 'Untitled2.ipynb',\n",
       " 'Untitled3.ipynb',\n",
       " 'Untitled4.ipynb',\n",
       " 'Untitled5.ipynb',\n",
       " 'Untitled6.ipynb',\n",
       " 'Untitled7.ipynb',\n",
       " 'Untitled8.ipynb',\n",
       " 'Untitled9.ipynb',\n",
       " 'Videos',\n",
       " 'yearly_summary.json',\n",
       " 'zip_file_1.zip',\n",
       " 'zip_file_2.zip',\n",
       " 'zip_file_3.zip',\n",
       " 'zip_file_4.zip']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a8e0434-0ecb-4e92-aac1-eb0f48b48fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All JSON files have been combined into 'combined.json'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "# List of your JSON filenames\n",
    "json_files = []\n",
    "combined_data = []\n",
    "# Loop through each file and append its content\n",
    "for file in json_files:\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                combined_data.extend(data)\n",
    "            else:\n",
    "                combined_data.append(data)\n",
    " \n",
    "with open(\"combined.json\", \"w\") as f:\n",
    "    json.dump(combined_data, f, indent=2)\n",
    " \n",
    "print(\"All JSON files have been combined into 'combined.json'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19d2ae85-ad4f-4c59-a2fb-e980c1f49dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV files combined into 'combined.csv'.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# List of CSV filenames\n",
    "csv_files = []\n",
    "\n",
    "# Open the output file\n",
    "with open(\"combined.csv\", \"w\", newline='') as outfile:\n",
    "    writer = None\n",
    "\n",
    "    for file in csv_files:\n",
    "        if file.endswith(\".csv\"):  # ✅ Only process .csv files\n",
    "            with open(file, \"r\") as infile:\n",
    "                reader = csv.reader(infile)\n",
    "                headers = next(reader)  # Read header\n",
    "\n",
    "                # Write header only once\n",
    "                if writer is None:\n",
    "                    writer = csv.writer(outfile)\n",
    "                    writer.writerow(headers)\n",
    "\n",
    "                # Write the rest of the rows\n",
    "                for row in reader:\n",
    "                    writer.writerow(row)\n",
    "\n",
    "print(\"✅ CSV files combined into 'combined.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70fa5dfe-2dac-4854-bf3f-8ba638f69b52",
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m dj=pd.read_json(\u001b[33m\"\u001b[39m\u001b[33mcombined.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dc=\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcombined.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "dj=pd.read_json(\"combined.json\")\n",
    "dc=pd.read_csv(\"combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee10563-1e0a-4172-8204-acf8f1ffde65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
